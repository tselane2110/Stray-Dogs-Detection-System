{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "data-preprocessing-2.ipynb",
      "authorship_tag": "ABX9TyMNuc5yky2BFQFTGgC5Ddi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tselane2110/stray-dogs-detection-system/blob/main/Model%20Training/data_preprocessing_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alyr2zyNycyj",
        "outputId": "3863e3eb-2c85-4757-efa0-bac59cc9c403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/horsedata.zip\n",
        "!unzip /content/cat.zip\n",
        "!unzip /content/human.zip"
      ],
      "metadata": {
        "id": "qQpXetRDAq-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "horse=\"/content/horsedata/images\"\n",
        "humans=\"/content/human/images\"\n",
        "\n",
        "for i in os.listdir(horse):\n",
        "    base = os.path.splitext(i)[0]\n",
        "    os.rename(\"/content/horsedata/images/\"+i, \"/content/horsedata/images/\" +base + '.jpg')\n",
        "\n",
        "\n",
        "for i in os.listdir(humans):\n",
        "    base = os.path.splitext(i)[0]\n",
        "    os.rename(\"/content/human/images/\"+i, \"/content/human/images/\" +base + '.jpg')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NBuQay6mykVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files = []\n",
        "# Add the path of txt folder\n",
        "for i in os.listdir(\"/content/cat/labels\"):\n",
        "    if i.endswith('.txt'):\n",
        "        files.append(i)\n",
        "\n",
        "for item in files:\n",
        "    # define an empty list\n",
        "    file_data = []\n",
        "\n",
        "    # open file and read the content in a list\n",
        "    with open(\"/content/cat/labels/\"+item, 'r') as myfile:\n",
        "      for line in myfile:\n",
        "        # remove linebreak which is the last character of the string\n",
        "        currentLine = line[:-1]\n",
        "        data = currentLine.split(\" \")\n",
        "        # add item to the list\n",
        "        file_data.append(data)\n",
        "\n",
        "    # Decrease the first number in any line by one\n",
        "    for i in file_data:\n",
        "        if type(i[0])==str:\n",
        "          temp = int(i[0]) + 1\n",
        "          i[0] = str(temp)\n",
        "\n",
        "    # Write back to the file\n",
        "    f = open(\"/content/cat/labels/\"+item, 'w')\n",
        "    for i in file_data:\n",
        "      res = \"\"\n",
        "      for j in i:\n",
        "        res += j + \" \"\n",
        "    f.write(res)\n",
        "    f.write(\"\\n\")\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "jFjspGTMXxBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files = []\n",
        "# Add the path of txt folder\n",
        "for i in os.listdir(\"/content/horsedata/labels\"):\n",
        "    if i.endswith('.txt'):\n",
        "        files.append(i)\n",
        "\n",
        "for item in files:\n",
        "    # define an empty list\n",
        "    file_data = []\n",
        "\n",
        "    # open file and read the content in a list\n",
        "    with open(\"/content/horsedata/labels/\"+item, 'r') as myfile:\n",
        "      for line in myfile:\n",
        "        # remove linebreak which is the last character of the string\n",
        "        currentLine = line[:-1]\n",
        "        data = currentLine.split(\" \")\n",
        "        # add item to the list\n",
        "        file_data.append(data)\n",
        "\n",
        "    # Decrease the first number in any line by one\n",
        "    for i in file_data:\n",
        "        if type(i[0])==str:\n",
        "          temp = int(i[0]) + 2\n",
        "          i[0] = str(temp)\n",
        "\n",
        "    # Write back to the file\n",
        "    f = open(\"/content/horsedata/labels/\"+item, 'w')\n",
        "    for i in file_data:\n",
        "      res = \"\"\n",
        "      for j in i:\n",
        "        res += j + \" \"\n",
        "    f.write(res)\n",
        "    f.write(\"\\n\")\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "HUGCGHoA0cCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files = []\n",
        "# Add the path of txt folder\n",
        "for i in os.listdir(\"/content/human/labels\"):\n",
        "    if i.endswith('.txt'):\n",
        "        files.append(i)\n",
        "\n",
        "for item in files:\n",
        "    # define an empty list\n",
        "    file_data = []\n",
        "\n",
        "    # open file and read the content in a list\n",
        "    with open(\"/content/human/labels/\"+item, 'r') as myfile:\n",
        "      for line in myfile:\n",
        "        # remove linebreak which is the last character of the string\n",
        "        currentLine = line[:-1]\n",
        "        data = currentLine.split(\" \")\n",
        "        # add item to the list\n",
        "        file_data.append(data)\n",
        "\n",
        "    # Decrease the first number in any line by one\n",
        "    for i in file_data:\n",
        "        if type(i[0])==str:\n",
        "          temp = int(i[0]) + 3\n",
        "          i[0] = str(temp)\n",
        "\n",
        "    # Write back to the file\n",
        "    f = open(\"/content/human/labels/\"+item, 'w')\n",
        "    for i in file_data:\n",
        "      res = \"\"\n",
        "      for j in i:\n",
        "        res += j + \" \"\n",
        "    f.write(res)\n",
        "    f.write(\"\\n\")\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "KIIlDvvg0sWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/human.zip -r /content/human\n",
        "!zip /content/horsedata.zip -r /content/horsedata\n",
        "!zip /content/cat.zip -r /content/cat"
      ],
      "metadata": {
        "id": "Fo6Nn-dD1N0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* deleting the images for which we dont have any annotation:\n"
      ],
      "metadata": {
        "id": "4OmxfXsZX4lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fypdataset2.zip"
      ],
      "metadata": {
        "id": "TXbJMdxlcb3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mi8awyg02UKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train, test, val split:"
      ],
      "metadata": {
        "id": "h6HwIx7cbjh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "annotation_root=\"/content/fypdataset2/labels\"\n",
        "root_path=\"/content/fypdataset2\"\n",
        "\n",
        "data = {}\n",
        "path_to_annotation_files = []\n",
        "path_to_image_files = []\n",
        "for i in os.listdir(annotation_root):\n",
        "  annotation_path = os.path.join(annotation_root, i)\n",
        "  y= re.split(\".txt\",i)\n",
        "  image_path = os.path.join(root_path, f\"images/{y[0]}.jpg\")\n",
        "  path_to_annotation_files.append(annotation_path)\n",
        "  path_to_image_files.append(image_path)\n",
        "\n",
        "\n",
        "print(\"no of images: \", len(path_to_annotation_files))\n",
        "print(\"no of labels: \", len(path_to_image_files))\n",
        "\n",
        "train_images, val_images, train_annotations, val_annotations= train_test_split(path_to_image_files, path_to_annotation_files,\n",
        "                                                                               test_size=0.2, random_state=1)\n",
        "\n",
        "val_images, test_images, val_annotations, test_annotations=train_test_split(val_images, val_annotations,\n",
        "                                                                           test_size=0.5, random_state=1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ZLEQgwbhUq",
        "outputId": "d9e630e3-d967-4545-f0b9-68875244872b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of images:  1990\n",
            "no of labels:  1990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create 2 folders as image1 and label1 in fypdataset2.\n",
        "2. create 3 folders, as training, testing and validation, in labels1 and image1 each, then execute the following code:"
      ],
      "metadata": {
        "id": "TUgNSuUecDax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def moves_files_to_folder(list_of_files, destination_folder):\n",
        "  for f in list_of_files:\n",
        "    try:\n",
        "      shutil.copy(f, destination_folder)\n",
        "    except:\n",
        "      print(f)\n",
        "      assert False\n",
        "\n",
        "moves_files_to_folder(train_images, \"/content/fypdataset2/images1/training\")\n",
        "moves_files_to_folder(val_images, \"/content/fypdataset2/images1/validation\")\n",
        "moves_files_to_folder(test_images, \"/content/fypdataset2/images1/testing\")\n",
        "moves_files_to_folder(train_annotations, \"/content/fypdataset2/labels1/training\")\n",
        "moves_files_to_folder(val_annotations, \"/content/fypdataset2/labels1/validation\")\n",
        "moves_files_to_folder(test_annotations, \"/content/fypdataset2/labels1/testing\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jTSZM__8b03P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"/content/fypdataset2/labels\")\n",
        "shutil.rmtree(\"/content/fypdataset2/images\")"
      ],
      "metadata": {
        "id": "x0RKN8DEvIpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/fypdataset2.zip -r /content/fypdataset2"
      ],
      "metadata": {
        "id": "vGvH29GwvId4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now cloning yolov5 repository to start with training our model"
      ],
      "metadata": {
        "id": "pWCu2Ykpa9gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZzmvLRglEuM",
        "outputId": "75ca6bb0-4e67-4445-8fb1-c4403441b007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 13145, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 13145 (delta 10), reused 13 (delta 4), pack-reused 13122\u001b[K\n",
            "Receiving objects: 100% (13145/13145), 12.53 MiB | 6.78 MiB/s, done.\n",
            "Resolving deltas: 100% (9023/9023), done.\n",
            "/content/yolov5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.13.0+cu113)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.64.0)\n",
            "Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (3.17.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.8.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (0.11.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 37)) (5.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (5.4.8)\n",
            "Collecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2207130030-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2022.6.15)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 12)) (4.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4.21.3->-r requirements.txt (line 15)) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.47.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 22)) (2022.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 37)) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r requirements.txt (line 37)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->-r requirements.txt (line 37)) (0.7.0)\n",
            "Installing collected packages: thop, PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 thop-0.1.1.post2207130030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training:"
      ],
      "metadata": {
        "id": "z4Dxon4UlcXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/best.pt --cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWtuUIT2lbbJ",
        "outputId": "c8af38f2-1960-450b-ac7d-7fb548c95fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/best.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.1-307-g92e47b8 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 27.0MB/s]\n",
            "Overriding model.yaml nc=1 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from /content/best.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset2/labels/training' images and labels...1591 found, 0 missing, 0 empty, 1 corrupt: 100% 1592/1592 [00:01<00:00, 1119.87it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset2/images/training/117.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset2/images/training/140.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/fypdataset2/labels/training.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.7GB ram): 100% 1591/1591 [00:09<00:00, 173.07it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset2/labels/validation' images and labels...199 found, 0 missing, 0 empty, 0 corrupt: 100% 199/199 [00:00<00:00, 463.62it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/fypdataset2/labels/validation.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB ram): 100% 199/199 [00:01<00:00, 182.64it/s]\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.19 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G   0.05023   0.02907     0.033        17       640: 100% 133/133 [31:59<00:00, 14.43s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:19<00:00,  8.79s/it]\n",
            "                 all        199        207    0.00678      0.997      0.362      0.198\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.03048   0.01935   0.02927        18       640: 100% 133/133 [32:33<00:00, 14.69s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.26s/it]\n",
            "                 all        199        207      0.309      0.659      0.443      0.221\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G   0.02745   0.01606   0.02612        17       640: 100% 133/133 [32:11<00:00, 14.53s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.25s/it]\n",
            "                 all        199        207      0.415      0.649      0.553      0.327\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G   0.02517   0.01585   0.02348        26       640: 100% 133/133 [32:21<00:00, 14.60s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.24s/it]\n",
            "                 all        199        207      0.735      0.559      0.548      0.299\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G    0.0257   0.01515   0.02027        23       640: 100% 133/133 [32:13<00:00, 14.54s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.32s/it]\n",
            "                 all        199        207      0.834      0.654      0.677      0.438\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.02449   0.01515   0.01736        16       640: 100% 133/133 [32:13<00:00, 14.54s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:15<00:00,  8.35s/it]\n",
            "                 all        199        207      0.675      0.792      0.788      0.454\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G   0.02292   0.01486   0.01295        21       640: 100% 133/133 [32:03<00:00, 14.46s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.27s/it]\n",
            "                 all        199        207      0.828      0.871      0.893      0.553\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G    0.0237   0.01497  0.009928        22       640: 100% 133/133 [32:10<00:00, 14.52s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.33s/it]\n",
            "                 all        199        207      0.872      0.885      0.924      0.567\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G   0.02212   0.01429   0.01003        15       640: 100% 133/133 [32:14<00:00, 14.55s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.32s/it]\n",
            "                 all        199        207      0.893      0.903      0.928      0.583\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G    0.0211   0.01432  0.008219        22       640: 100% 133/133 [32:14<00:00, 14.54s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.30s/it]\n",
            "                 all        199        207      0.891      0.932      0.944      0.615\n",
            "\n",
            "10 epochs completed in 5.582 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:14<00:00,  8.27s/it]\n",
            "                 all        199        207      0.891      0.932      0.944      0.615\n",
            "                 dog        199         78      0.833      0.872       0.91      0.652\n",
            "                 cat        199         28      0.797      0.857      0.875      0.493\n",
            "               horse        199         40      0.988          1      0.995      0.709\n",
            "               human        199         61      0.947          1      0.995      0.604\n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/exp2.zip -r /content/yolov5/runs/train/exp2"
      ],
      "metadata": {
        "id": "S_zkvbxSwg35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 640 --conf 0.5 --source \"/content/1.jpeg\"\n",
        "# for inference\n",
        "# detecting new images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sRtd5Er-ACs",
        "outputId": "ce662ad0-b0f0-48b1-9fb2-6c2bda70904f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/exp2/weights/best.pt'], source=/content/1.jpeg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.1-307-g92e47b8 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/1 /content/1.jpeg: 640x320 Done. (0.209s)\n",
            "Speed: 4.0ms pre-process, 209.4ms inference, 0.5ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 640 --conf 0.5 --source \"/content/fypdataset2/images/testing\"\n",
        "# for inference\n",
        "# detecting new images"
      ],
      "metadata": {
        "id": "v-OQaYpV-Kj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 640 --conf 0.5 --source \"/content/video1.mp4\"\n",
        "# for inference\n",
        "# detecting new images"
      ],
      "metadata": {
        "id": "oZqSH6wG-fLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --data /content/yolov5/data/custom_dataset.yaml --img 640"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbblTKMncM_H",
        "outputId": "cf8122bf-dd35-449e-85ec-4ab96f28bbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/custom_dataset.yaml, weights=['/content/yolov5/runs/train/exp2/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.1-307-g92e47b8 Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset2/labels/validation.cache' images and labels... 199 found, 0 missing, 0 empty, 0 corrupt: 100% 199/199 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [01:20<00:00, 11.48s/it]\n",
            "                 all        199        207      0.878      0.941      0.946      0.619\n",
            "                 dog        199         78      0.806      0.897      0.912      0.652\n",
            "                 cat        199         28      0.776      0.865      0.882      0.513\n",
            "               horse        199         40      0.987          1      0.995       0.71\n",
            "               human        199         61      0.943          1      0.995      0.603\n",
            "Speed: 4.1ms pre-process, 393.3ms inference, 0.7ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 640 --conf 0.5 --source \"/content/video5.mp4\""
      ],
      "metadata": {
        "id": "okKh96NJdE2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --img 640 --source /content/download.jpg --conf 0.5 --weights /content/best.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-7HJIB6pDr4",
        "outputId": "a34cf0e8-8126-474e-d0a9-d9dcd2ebb1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/best.pt'], source=/content/download.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.1-309-g4c1784b Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/1 /content/download.jpg: 448x640 Done. (0.289s)\n",
            "Speed: 2.5ms pre-process, 289.4ms inference, 0.4ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --img 640 --source /content/horse1.mp4 --conf 0.5 --weights /content/best.pt"
      ],
      "metadata": {
        "id": "njr7Ks1zS-gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fypdataset2.zip"
      ],
      "metadata": {
        "id": "ca8sgauqxLfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 12 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/best.pt --cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYnJA8E3q5uI",
        "outputId": "43d9aabc-8ead-480d-8dc1-8efaa5311de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/best.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=12, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.1-309-g4c1784b Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from /content/best.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/content/fypdataset2/labels/training' images and labels...1591 found, 0 missing, 0 empty, 1 corrupt: 100% 1592/1592 [00:01<00:00, 1047.93it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/yolov5/content/fypdataset2/images/training/117.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/yolov5/content/fypdataset2/images/training/140.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/content/fypdataset2/labels/training.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.7GB ram): 100% 1591/1591 [00:09<00:00, 170.81it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/content/fypdataset2/labels/validation' images and labels...199 found, 0 missing, 0 empty, 0 corrupt: 100% 199/199 [00:00<00:00, 503.95it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/content/fypdataset2/labels/validation.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB ram): 100% 199/199 [00:01<00:00, 161.98it/s]\n",
            "Plotting labels to runs/train/exp3/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.19 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G   0.01999   0.01428    0.0072        17       640: 100% 133/133 [35:18<00:00, 15.93s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:29<00:00,  9.96s/it]\n",
            "                 all        199        207      0.914      0.948      0.948      0.641\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.02142   0.01402  0.007224        18       640: 100% 133/133 [35:26<00:00, 15.99s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:29<00:00,  9.89s/it]\n",
            "                 all        199        207      0.878      0.878      0.907      0.471\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G    0.0241    0.0138  0.007137        17       640: 100% 133/133 [35:16<00:00, 15.92s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:26<00:00,  9.60s/it]\n",
            "                 all        199        207      0.864      0.853      0.895      0.477\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G   0.02288   0.01416  0.008152        26       640: 100% 133/133 [35:44<00:00, 16.12s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:26<00:00,  9.66s/it]\n",
            "                 all        199        207       0.87      0.915      0.912      0.576\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G   0.02493   0.01403  0.008344        23       640: 100% 133/133 [36:09<00:00, 16.31s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:29<00:00,  9.98s/it]\n",
            "                 all        199        207      0.872      0.912      0.919      0.555\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.02377    0.0144  0.007107        16       640: 100% 133/133 [36:25<00:00, 16.44s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:27<00:00,  9.74s/it]\n",
            "                 all        199        207      0.838      0.905      0.909      0.543\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G   0.02123   0.01399  0.006632        21       640: 100% 133/133 [36:41<00:00, 16.55s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:27<00:00,  9.75s/it]\n",
            "                 all        199        207      0.839      0.844      0.886      0.551\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G   0.02214   0.01422  0.005089        22       640: 100% 133/133 [37:01<00:00, 16.70s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:29<00:00,  9.98s/it]\n",
            "                 all        199        207      0.916      0.934      0.941      0.622\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G   0.01988   0.01357   0.00568        15       640: 100% 133/133 [37:46<00:00, 17.04s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:33<00:00, 10.34s/it]\n",
            "                 all        199        207      0.917      0.921      0.935      0.624\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G   0.01906   0.01361  0.005139        22       640: 100% 133/133 [37:30<00:00, 16.92s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:30<00:00, 10.02s/it]\n",
            "                 all        199        207      0.936      0.915      0.935      0.617\n",
            "\n",
            "10 epochs completed in 6.305 hours.\n",
            "Optimizer stripped from runs/train/exp3/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp3/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp3/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 9/9 [01:27<00:00,  9.71s/it]\n",
            "                 all        199        207      0.914      0.948      0.948      0.643\n",
            "                 dog        199         78      0.821      0.897      0.918      0.672\n",
            "                 cat        199         28      0.859      0.893      0.886      0.575\n",
            "               horse        199         40      0.985          1      0.995      0.718\n",
            "               human        199         61      0.991          1      0.995      0.605\n",
            "Results saved to \u001b[1mruns/train/exp3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/exp7 -r /content/yolov5/runs/train/exp3"
      ],
      "metadata": {
        "id": "l7uXV1YRxmiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --img 640 --source /content/horse1.mp4 --conf 0.5 --weights /content/yolov5/runs/train/exp3/weights/best.pt"
      ],
      "metadata": {
        "id": "weTqnOvjJ_VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello world!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zlIu6gBOzNw",
        "outputId": "67598657-5565-47f2-cf02-1f27f779ffc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just execute !unzip without any file to know bout more features XD\n",
        "!unzip /content/fypdataset3.zip"
      ],
      "metadata": {
        "id": "yjt-SunsO1ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/human2.zip"
      ],
      "metadata": {
        "id": "uP2SAiuz5hE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting png files to jpg\n",
        "\n",
        "from PIL import Image\n",
        "path='/content/human/images1/'\n",
        "path2= '/content/human/images/'\n",
        "\n",
        "for i in os.listdir(path):\n",
        "  img_png = Image.open(path+i)\n",
        "  rgb_im = img_png.convert('RGB')\n",
        "  text=i                           # we need i == n02110806_3971.jpg\n",
        "  d=re.search ('\\d+', text)\n",
        "  if d.group(0)!=None:\n",
        "    b=d.group(0)\n",
        "    x=path2+b+\".jpg\"\n",
        "  rgb_im.save(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "GxOzjoIt92X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import re, os\n",
        "\n",
        "annotation_root=\"/content/human/labels\"\n",
        "root_path=\"/content/human\"\n",
        "\n",
        "data = {}\n",
        "path_to_annotation_files = []\n",
        "path_to_image_files = []\n",
        "for i in os.listdir(annotation_root):\n",
        "  annotation_path = os.path.join(annotation_root, i)\n",
        "  y= re.split(\".txt\",i)\n",
        "  image_path = os.path.join(root_path, f\"images/{y[0]}.jpg\")\n",
        "  path_to_annotation_files.append(annotation_path)\n",
        "  path_to_image_files.append(image_path)\n",
        "\n",
        "\n",
        "print(\"no of images: \", len(path_to_annotation_files))\n",
        "print(\"no of labels: \", len(path_to_image_files))\n",
        "\n",
        "train_images, val_images, train_annotations, val_annotations= train_test_split(path_to_image_files, path_to_annotation_files,\n",
        "                                                                               test_size=0.2, random_state=25)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUO1at1WRwU8",
        "outputId": "bfacd8fb-af92-48e1-e91d-78127c6a6751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of images:  551\n",
            "no of labels:  551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def moves_files_to_folder(list_of_files, destination_folder):\n",
        "  for f in list_of_files:\n",
        "    try:\n",
        "      shutil.copy(f, destination_folder)\n",
        "    except:\n",
        "      print(f)\n",
        "      assert False\n",
        "\n",
        "moves_files_to_folder(train_images, \"/content/human/images/training\")\n",
        "moves_files_to_folder(val_images, \"/content/human/images/validation\")\n",
        "moves_files_to_folder(train_annotations, \"/content/human/labels/training\")\n",
        "moves_files_to_folder(val_annotations, \"/content/human/labels/validation\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Vv7I34uzR8tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/human -r /content/human"
      ],
      "metadata": {
        "id": "hqNvP0Gefkqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "0d5cEsW0RHQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 8 --epochs 10  --data /content/yolov5/data/custom_dataset.yaml --weights /content/best.pt --cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgQvCtQKRZGB",
        "outputId": "14198b03-e6c1-46e1-f004-1daa1fe4f80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/best.pt, cfg=, data=/content/yolov5/data/custom_dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=8, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.1-311-gb17629e Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 270 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from /content/best.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/human/labels/training' images and labels...440 found, 0 missing, 0 empty, 0 corrupt: 100% 440/440 [00:00<00:00, 1583.33it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/human/labels/training.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram): 100% 440/440 [00:05<00:00, 75.13it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/human/labels/validation' images and labels...111 found, 0 missing, 0 empty, 0 corrupt: 100% 111/111 [00:00<00:00, 619.12it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/human/labels/validation.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 111/111 [00:02<00:00, 40.49it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.97 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9        0G   0.05531   0.03711   0.04409        35       640: 100% 55/55 [10:03<00:00, 10.97s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:53<00:00,  7.68s/it]\n",
            "                 all        111        164      0.384      0.303      0.328      0.143\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9        0G   0.05285   0.02992   0.01609        17       640: 100% 55/55 [09:48<00:00, 10.70s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:50<00:00,  7.15s/it]\n",
            "                 all        111        164      0.848      0.329      0.341      0.125\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9        0G   0.05216   0.02707  0.008712        23       640: 100% 55/55 [09:52<00:00, 10.77s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:50<00:00,  7.19s/it]\n",
            "                 all        111        164      0.931      0.316      0.369      0.147\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9        0G   0.05149   0.02522  0.007469        39       640: 100% 55/55 [09:54<00:00, 10.81s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:50<00:00,  7.26s/it]\n",
            "                 all        111        164      0.868      0.343      0.363      0.145\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9        0G   0.05108   0.02444  0.006117        43       640: 100% 55/55 [09:51<00:00, 10.76s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:50<00:00,  7.24s/it]\n",
            "                 all        111        164      0.919      0.353      0.419      0.185\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9        0G   0.04504   0.02327  0.004602        17       640: 100% 55/55 [09:52<00:00, 10.77s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:51<00:00,  7.31s/it]\n",
            "                 all        111        164      0.386      0.585      0.488      0.205\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9        0G    0.0447   0.02457  0.004902        32       640: 100% 55/55 [09:50<00:00, 10.74s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:51<00:00,  7.29s/it]\n",
            "                 all        111        164      0.457      0.356      0.384       0.19\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9        0G   0.04283   0.02168  0.004567        18       640: 100% 55/55 [09:51<00:00, 10.76s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:49<00:00,  7.11s/it]\n",
            "                 all        111        164      0.439      0.353      0.386       0.19\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9        0G   0.04342   0.02489  0.002996        37       640: 100% 55/55 [10:04<00:00, 10.98s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:50<00:00,  7.27s/it]\n",
            "                 all        111        164      0.457      0.347      0.388      0.203\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9        0G   0.04124   0.02322  0.002445        19       640: 100% 55/55 [10:00<00:00, 10.92s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:50<00:00,  7.18s/it]\n",
            "                 all        111        164      0.469      0.353      0.394      0.214\n",
            "\n",
            "10 epochs completed in 1.796 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.5MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.5MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:49<00:00,  7.06s/it]\n",
            "                 all        111        164      0.388      0.595      0.488      0.205\n",
            "                 Dog        111          1      0.087      0.435      0.199     0.0207\n",
            "               Human        111        163      0.688      0.755      0.777      0.388\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/yolov5/yolov5/runs/train/exp/weights/best.pt --data /content/yolov5/data/custom_dataset.yaml --img 640"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJVAXsIoRe5S",
        "outputId": "cc14a76b-8314-4dec-bd12-213bc0f0e9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/custom_dataset.yaml, weights=['/content/yolov5/yolov5/runs/train/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.1-311-gb17629e Python-3.7.13 torch-1.12.0+cu113 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 213 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/human/labels/validation.cache' images and labels... 111 found, 0 missing, 0 empty, 0 corrupt: 100% 111/111 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:55<00:00, 13.97s/it]\n",
            "                 all        111        164      0.387      0.586      0.488      0.204\n",
            "                 Dog        111          1     0.0837      0.418      0.199     0.0207\n",
            "               Human        111        163       0.69      0.755      0.777      0.386\n",
            "Speed: 9.2ms pre-process, 479.8ms inference, 1.5ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fypdataset_gpu.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X123EseGArPS",
        "outputId": "ffe2ee5b-75b4-443e-d95f-2700c487ab67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fypdataset_gpu.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/fypdataset_gpu.zip or\n",
            "        /content/fypdataset_gpu.zip.zip, and cannot find /content/fypdataset_gpu.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.call([\"/content/yolov5/data/scripts/download_weights.sh\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJIE8w3FBvs3",
        "outputId": "9a07c87e-cfc6-4936-9d44-e3c6e6a5be20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --cfg /content/yolov5/models/yolov5m.yaml --hyp /content/yolov5/data/hyps/hyp.scratch-med.yaml --batch 32 --epochs 10 --data /content/yolov5/data/custom_dataset.yaml --weights  /content/best.pt --workers 24"
      ],
      "metadata": {
        "id": "FqovmUWPRfDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n01YsSM7RfFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import re, os\n",
        "\n",
        "annotation_root=\"/content/fypdataset3/labels\"\n",
        "root_path=\"/content/fypdataset3\"\n",
        "\n",
        "data = {}\n",
        "path_to_annotation_files = []\n",
        "path_to_image_files = []\n",
        "for i in os.listdir(annotation_root):\n",
        "  annotation_path = os.path.join(annotation_root, i)\n",
        "  y= re.split(\".txt\",i)\n",
        "  image_path = os.path.join(root_path, f\"images/{y[0]}.jpg\")\n",
        "  path_to_annotation_files.append(annotation_path)\n",
        "  path_to_image_files.append(image_path)\n",
        "\n",
        "\n",
        "print(\"no of images: \", len(path_to_annotation_files))\n",
        "print(\"no of labels: \", len(path_to_image_files))\n",
        "\n",
        "train_images, val_images, train_annotations, val_annotations= train_test_split(path_to_image_files, path_to_annotation_files,\n",
        "                                                                               test_size=0.2, random_state=25)\n",
        "\n",
        "val_images, test_images, val_annotations, test_annotations=train_test_split(val_images, val_annotations,\n",
        "                                                                           test_size=0.5, random_state=25)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8hHn_rk8VkF",
        "outputId": "1a7c917b-b9ac-427e-b677-3c362b0e0c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of images:  3483\n",
            "no of labels:  3483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def moves_files_to_folder(list_of_files, destination_folder):\n",
        "  for f in list_of_files:\n",
        "    try:\n",
        "      shutil.copy(f, destination_folder)\n",
        "    except:\n",
        "      print(f)\n",
        "      assert False\n",
        "\n",
        "moves_files_to_folder(train_images, \"/content/fypdataset3/images1/training\")\n",
        "moves_files_to_folder(val_images, \"/content/fypdataset3/images1/validation\")\n",
        "moves_files_to_folder(test_images, \"/content/fypdataset3/images1/testing\")\n",
        "moves_files_to_folder(train_annotations, \"/content/fypdataset3/labels1/training\")\n",
        "moves_files_to_folder(val_annotations, \"/content/fypdataset3/labels1/validation\")\n",
        "moves_files_to_folder(test_annotations, \"/content/fypdataset3/labels1/testing\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LViCeEAm9UqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for image training data:\n",
        "import os\n",
        "\n",
        "path= \"/content/fypdataset3/images/training\"\n",
        "j=0\n",
        "chunki1=[]\n",
        "chunki2=[]\n",
        "chunki3=[]\n",
        "\n",
        "\n",
        "for i in os.listdir(path):\n",
        "  if j>=0 and j<1161:\n",
        "    chunki1.append(i)\n",
        "\n",
        "  if j>=1161 and j<2322:\n",
        "    chunki2.append(i)\n",
        "\n",
        "  if j>=2322 and j<3483:\n",
        "    chunki3.append(i)\n",
        "\n",
        "  j+=1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GPRQ0aD5ACPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now divide the data into content/fypdataset/images/training1, training2, training3, training4\n",
        "newchunki1=[]\n",
        "newchunki2=[]\n",
        "newchunki3=[]\n",
        "\n",
        "for i in chunki1:\n",
        "  newchunki1.append(\"/content/fypdataset3/images/training/\"+i)\n",
        "\n",
        "for i in chunki2:\n",
        "  newchunki2.append(\"/content/fypdataset3/images/training/\"+i)\n",
        "\n",
        "for i in chunki3:\n",
        "  newchunki3.append(\"/content/fypdataset3/images/training/\"+i)\n",
        "\n",
        "moves_files_to_folder(newchunki1, \"/content/fypdataset3/images/training1\")\n",
        "moves_files_to_folder(newchunki2, \"/content/fypdataset3/images/training2\")\n",
        "moves_files_to_folder(newchunki3, \"/content/fypdataset3/images/training3\")\n"
      ],
      "metadata": {
        "id": "bniYnSFKAdAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for labels training data:\n",
        "\n",
        "\n",
        "chunkl1=[]\n",
        "chunkl2=[]\n",
        "chunkl3=[]\n",
        "\n",
        "for i in chunki1:\n",
        "  base = os.path.splitext(i)[0]\n",
        "  chunkl1.append(base+\".txt\")\n",
        "\n",
        "for i in chunki2:\n",
        "  base = os.path.splitext(i)[0]\n",
        "  chunkl2.append(base+\".txt\")\n",
        "\n",
        "for i in chunki3:\n",
        "  base = os.path.splitext(i)[0]\n",
        "  chunkl3.append(base+\".txt\")\n"
      ],
      "metadata": {
        "id": "duV9Qy4pBKZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now divide the data into content/fypdataset/labels/training1, training2, training3, training4\n",
        "\n",
        "newchunkl1=[]\n",
        "newchunkl2=[]\n",
        "newchunkl3=[]\n",
        "newchunkl4=[]\n",
        "\n",
        "for i in chunkl1:\n",
        "  newchunkl1.append(\"/content/fypdataset3/labels/training/\"+i)\n",
        "\n",
        "\n",
        "for i in chunkl2:\n",
        "  newchunkl2.append(\"/content/fypdataset3/labels/training/\"+i)\n",
        "\n",
        "\n",
        "for i in chunkl3:\n",
        "  newchunkl3.append(\"/content/fypdataset3/labels/training/\"+i)\n",
        "\n",
        "moves_files_to_folder(newchunkl1, \"/content/fypdataset3/labels/training1\")\n",
        "moves_files_to_folder(newchunkl2, \"/content/fypdataset3/labels/training2\")\n",
        "moves_files_to_folder(newchunkl3, \"/content/fypdataset3/labels/training3\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pnAYBYjyBXwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/fypdataset3 -r /content/fypdataset3"
      ],
      "metadata": {
        "id": "31apkQSHDA6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fypdataset_gpu.zip"
      ],
      "metadata": {
        "id": "r9vCAEoVPrlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Yt3Pm02YEwPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --cfg /content/yolov5/models/yolov5m.yaml --hyp /content/yolov5/data/hyps/hyp.scratch-med.yaml --batch 32 --epochs 20 --data /content/yolov5/data/custom_dataset.yaml --weights  /content/best.pt --workers 24"
      ],
      "metadata": {
        "id": "12vI_wje_jm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22985038-0053-4401-a892-b0bdac480a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/best.pt, cfg=/content/yolov5/models/yolov5m.yaml, data=/content/yolov5/data/custom_dataset.yaml, hyp=/content/yolov5/data/hyps/hyp.scratch-med.yaml, epochs=20, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=24, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 3 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 🚀 v6.1-321-g2e1291f Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     36369  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "YOLOv5m summary: 369 layers, 20883441 parameters, 20883441 gradients, 48.3 GFLOPs\n",
            "\n",
            "Transferred 480/481 items from /content/best.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 79 weight (no decay), 82 weight, 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset/labels/training.cache' images and labels... 8119 found, 0 missing, 0 empty, 2 corrupt: 100% 8121/8121 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/10026.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/8470.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/8577.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/8646.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/9565.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/9815.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/n02089973_1763.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/n02089973_2054.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset/labels/validation.cache' images and labels... 407 found, 0 missing, 0 empty, 0 corrupt: 100% 407/407 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/fypdataset/images/validation/9297.jpg: corrupt JPEG restored and saved\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.57 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/19     12.1G   0.02083  0.009624  0.001413        76       640: 100% 254/254 [04:16<00:00,  1.01s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:07<00:00,  1.10s/it]\n",
            "                 all        407        444      0.941      0.936      0.967      0.716\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/19     10.8G   0.02367   0.01018  0.002033        64       640: 100% 254/254 [04:07<00:00,  1.02it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.19it/s]\n",
            "                 all        407        444      0.967       0.92      0.957      0.686\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/19     10.8G   0.02454   0.01032  0.002151        75       640: 100% 254/254 [04:03<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.17it/s]\n",
            "                 all        407        444        0.9       0.93      0.933      0.646\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/19     10.8G   0.02678   0.01109  0.002809        75       640: 100% 254/254 [04:04<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.19it/s]\n",
            "                 all        407        444      0.925      0.919      0.937      0.637\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/19     11.8G   0.02676   0.01132  0.003301        78       640: 100% 254/254 [04:07<00:00,  1.03it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.18it/s]\n",
            "                 all        407        444      0.869      0.772       0.86      0.552\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/19     11.8G   0.02696   0.01135  0.003207        66       640: 100% 254/254 [04:02<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.18it/s]\n",
            "                 all        407        444      0.931      0.841      0.934      0.634\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/19     11.8G   0.02631   0.01122  0.002707        75       640: 100% 254/254 [04:01<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.13it/s]\n",
            "                 all        407        444      0.955      0.882      0.938      0.645\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/19     11.8G   0.02573   0.01105  0.002723        62       640: 100% 254/254 [04:01<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.16it/s]\n",
            "                 all        407        444      0.926      0.931      0.954      0.679\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/19     11.8G    0.0256    0.0109  0.002572        74       640: 100% 254/254 [04:02<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.09it/s]\n",
            "                 all        407        444      0.931      0.912      0.958      0.672\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/19     11.8G    0.0251    0.0109  0.002638        64       640: 100% 254/254 [04:05<00:00,  1.03it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.06it/s]\n",
            "                 all        407        444      0.958      0.922      0.958      0.674\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/19     11.8G   0.02455   0.01068  0.002472        78       640: 100% 254/254 [04:04<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.11it/s]\n",
            "                 all        407        444      0.948      0.907      0.949      0.654\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/19     11.8G   0.02438   0.01071  0.002536        65       640: 100% 254/254 [04:04<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.13it/s]\n",
            "                 all        407        444      0.954      0.931      0.966      0.681\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/19     11.8G   0.02397   0.01075  0.002227        65       640: 100% 254/254 [04:03<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.21it/s]\n",
            "                 all        407        444      0.956      0.915      0.956      0.686\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/19     11.8G   0.02313   0.01047  0.002326        67       640: 100% 254/254 [04:03<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.14it/s]\n",
            "                 all        407        444      0.931      0.923      0.956      0.689\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/19     11.8G   0.02286   0.01018  0.001991        91       640: 100% 254/254 [04:01<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.19it/s]\n",
            "                 all        407        444      0.927      0.934      0.968      0.693\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     15/19     11.8G   0.02228   0.01006  0.001844        81       640: 100% 254/254 [04:03<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.10it/s]\n",
            "                 all        407        444      0.933      0.938       0.96      0.691\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     16/19     11.8G    0.0225   0.01006  0.001788        75       640: 100% 254/254 [04:04<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.19it/s]\n",
            "                 all        407        444      0.951      0.938      0.964      0.699\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     17/19     11.8G   0.02211   0.00993  0.001669        58       640: 100% 254/254 [04:02<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.18it/s]\n",
            "                 all        407        444      0.948      0.924      0.961       0.71\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     18/19     11.8G   0.02166  0.009948  0.001639        75       640: 100% 254/254 [04:02<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.15it/s]\n",
            "                 all        407        444      0.949      0.921      0.962      0.702\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     19/19     11.8G   0.02162  0.009787  0.001741        75       640: 100% 254/254 [04:02<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.19it/s]\n",
            "                 all        407        444      0.951      0.941      0.972      0.711\n",
            "\n",
            "20 epochs completed in 1.396 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 42.2MB\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 42.2MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "YOLOv5m summary: 290 layers, 20865057 parameters, 0 gradients, 47.9 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.03it/s]\n",
            "                 all        407        444      0.941      0.936      0.967      0.715\n",
            "                 Dog        407        303      0.951      0.967      0.979       0.82\n",
            "                 Cat        407         75      0.926      0.837      0.946      0.604\n",
            "               Horse        407         17       0.97          1      0.995      0.889\n",
            "               Human        407         49      0.919      0.939      0.947      0.549\n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/exp2 -r /content/yolov5/runs/train/exp2"
      ],
      "metadata": {
        "id": "EzIZAii2Ca2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --cfg /content/yolov5/models/yolov5m.yaml --hyp /content/yolov5/data/hyps/hyp.scratch-med.yaml --batch 32 --epochs 20 --data /content/yolov5/data/custom_dataset.yaml --weights /content/yolov5/runs/train/exp2/weights/best.pt  --workers 24"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h_zJ6k3uk3a",
        "outputId": "843ae7ad-e380-483c-8959-efdbe2dca4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov5/runs/train/exp2/weights/best.pt, cfg=/content/yolov5/models/yolov5m.yaml, data=/content/yolov5/data/custom_dataset.yaml, hyp=/content/yolov5/data/hyps/hyp.scratch-med.yaml, epochs=20, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=24, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 3 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 🚀 v6.1-321-g2e1291f Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     36369  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "YOLOv5m summary: 369 layers, 20883441 parameters, 20883441 gradients, 48.3 GFLOPs\n",
            "\n",
            "Transferred 480/481 items from /content/yolov5/runs/train/exp2/weights/best.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 79 weight (no decay), 82 weight, 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset/labels/validation.cache' images and labels... 407 found, 0 missing, 0 empty, 0 corrupt: 100% 407/407 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/fypdataset/images/validation/9297.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset/labels/testing' images and labels...1623 found, 0 missing, 0 empty, 1 corrupt: 100% 1624/1624 [00:03<00:00, 482.14it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/fypdataset/images/testing/9778.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/fypdataset/labels/testing.cache\n",
            "Plotting labels to runs/train/exp3/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.54 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/19     12.1G    0.0244   0.01133  0.003014        58       640: 100% 13/13 [00:13<00:00,  1.04s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:21<00:00,  1.22it/s]\n",
            "                 all       1623       1773      0.955      0.935      0.956      0.699\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/19     10.8G   0.02331   0.01028  0.003395        63       640: 100% 13/13 [00:10<00:00,  1.22it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:19<00:00,  1.31it/s]\n",
            "                 all       1623       1773      0.948      0.938      0.954      0.696\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/19     10.8G   0.02574   0.01121  0.003299        88       640: 100% 13/13 [00:10<00:00,  1.22it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:19<00:00,  1.31it/s]\n",
            "                 all       1623       1773      0.955      0.934      0.956      0.697\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/19     10.8G   0.02413   0.01097  0.003201        85       640: 100% 13/13 [00:10<00:00,  1.23it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.28it/s]\n",
            "                 all       1623       1773      0.943       0.93      0.954      0.684\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/19     10.8G   0.02493   0.01054  0.002834        78       640: 100% 13/13 [00:10<00:00,  1.19it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.28it/s]\n",
            "                 all       1623       1773      0.943      0.934      0.953      0.693\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/19     10.8G   0.02392  0.009962  0.003128        75       640: 100% 13/13 [00:10<00:00,  1.25it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:19<00:00,  1.30it/s]\n",
            "                 all       1623       1773      0.927      0.921      0.942      0.672\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/19     10.8G   0.02612   0.01069  0.002767        67       640: 100% 13/13 [00:10<00:00,  1.28it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.27it/s]\n",
            "                 all       1623       1773      0.912      0.895       0.92      0.608\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/19     10.8G   0.02617   0.01044  0.003003        64       640: 100% 13/13 [00:10<00:00,  1.25it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.26it/s]\n",
            "                 all       1623       1773      0.909      0.899      0.923       0.62\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/19     10.8G   0.02507   0.01021  0.002748        64       640: 100% 13/13 [00:10<00:00,  1.28it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.29it/s]\n",
            "                 all       1623       1773      0.925       0.91       0.94      0.663\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/19     10.8G   0.02598  0.009699  0.003456        61       640: 100% 13/13 [00:10<00:00,  1.27it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.29it/s]\n",
            "                 all       1623       1773       0.92      0.911      0.936      0.649\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/19     10.8G   0.02412  0.009763  0.002568        64       640: 100% 13/13 [00:10<00:00,  1.27it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.28it/s]\n",
            "                 all       1623       1773      0.927      0.907      0.939      0.648\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/19     10.8G   0.02425   0.01059  0.003598        82       640: 100% 13/13 [00:10<00:00,  1.27it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.25it/s]\n",
            "                 all       1623       1773      0.931      0.902      0.928      0.638\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/19     10.8G   0.02486   0.01091  0.003136        74       640: 100% 13/13 [00:10<00:00,  1.27it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.27it/s]\n",
            "                 all       1623       1773      0.937      0.909      0.936      0.654\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/19     10.8G   0.02468   0.01061  0.003102        70       640: 100% 13/13 [00:10<00:00,  1.24it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.29it/s]\n",
            "                 all       1623       1773      0.922      0.905      0.934      0.644\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/19     10.8G   0.02432   0.01087  0.003911        71       640: 100% 13/13 [00:10<00:00,  1.25it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.29it/s]\n",
            "                 all       1623       1773      0.933      0.899      0.939      0.661\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     15/19     10.8G   0.02409   0.00971  0.002872        73       640: 100% 13/13 [00:10<00:00,  1.24it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.28it/s]\n",
            "                 all       1623       1773      0.933      0.908       0.94      0.664\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     16/19     10.8G   0.02326   0.01026  0.002575        78       640: 100% 13/13 [00:10<00:00,  1.27it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.29it/s]\n",
            "                 all       1623       1773      0.926      0.904      0.933      0.656\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     17/19     10.8G   0.02173  0.009878  0.001914        90       640: 100% 13/13 [00:10<00:00,  1.27it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.29it/s]\n",
            "                 all       1623       1773      0.931      0.909      0.934      0.653\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     18/19     10.8G   0.02188  0.009428  0.001715        75       640: 100% 13/13 [00:10<00:00,  1.26it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.26it/s]\n",
            "                 all       1623       1773      0.918      0.917      0.938      0.664\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     19/19     10.8G   0.02199   0.01013  0.002035        84       640: 100% 13/13 [00:10<00:00,  1.27it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:20<00:00,  1.27it/s]\n",
            "                 all       1623       1773       0.93      0.915      0.939      0.668\n",
            "\n",
            "20 epochs completed in 0.176 hours.\n",
            "Optimizer stripped from runs/train/exp3/weights/last.pt, 42.2MB\n",
            "Optimizer stripped from runs/train/exp3/weights/best.pt, 42.2MB\n",
            "\n",
            "Validating runs/train/exp3/weights/best.pt...\n",
            "Fusing layers... \n",
            "YOLOv5m summary: 290 layers, 20865057 parameters, 0 gradients, 47.9 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:21<00:00,  1.20it/s]\n",
            "                 all       1623       1773      0.955      0.935      0.956      0.699\n",
            "                 Dog       1623       1187      0.959       0.97      0.985       0.84\n",
            "                 Cat       1623        309      0.949      0.922      0.957       0.64\n",
            "               Horse       1623         74      0.995          1      0.995      0.823\n",
            "               Human       1623        203      0.919      0.847      0.886      0.494\n",
            "Results saved to \u001b[1mruns/train/exp3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip /content/exp3 -r /content/yolov5/runs/train/exp3"
      ],
      "metadata": {
        "id": "Nl29yOVe32uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --cfg /content/yolov5/models/yolov5m.yaml --hyp /content/yolov5/data/hyps/hyp.scratch-med.yaml --batch 32 --epochs 20 --data /content/yolov5/data/custom_dataset.yaml --weights /content/yolov5/runs/train/exp2/weights/best.pt  --workers 24"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoX1p3b91FDr",
        "outputId": "479bff7f-e90b-4a10-b1ea-8c3a9c6e730c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov5/runs/train/exp2/weights/best.pt, cfg=/content/yolov5/models/yolov5m.yaml, data=/content/yolov5/data/custom_dataset.yaml, hyp=/content/yolov5/data/hyps/hyp.scratch-med.yaml, epochs=20, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=24, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 3 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 🚀 v6.1-321-g2e1291f Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     36369  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "YOLOv5m summary: 369 layers, 20883441 parameters, 20883441 gradients, 48.3 GFLOPs\n",
            "\n",
            "Transferred 480/481 items from /content/yolov5/runs/train/exp2/weights/best.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 79 weight (no decay), 82 weight, 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/fypdataset/labels/training.cache' images and labels... 8119 found, 0 missing, 0 empty, 2 corrupt: 100% 8121/8121 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/10026.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/8470.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/8577.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/8646.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/9565.jpg: ignoring corrupt image/label: invalid image format GIF\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/9815.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/n02089973_1763.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/fypdataset/images/training/n02089973_2054.jpg: corrupt JPEG restored and saved\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/fypdataset/labels/validation.cache' images and labels... 407 found, 0 missing, 0 empty, 0 corrupt: 100% 407/407 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING: /content/fypdataset/images/validation/9297.jpg: corrupt JPEG restored and saved\n",
            "Plotting labels to runs/train/exp4/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.57 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp4\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/19     12.1G   0.01963  0.009299  0.001088        76       640: 100% 254/254 [04:10<00:00,  1.01it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.08it/s]\n",
            "                 all        407        444      0.958      0.931      0.962        0.7\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/19     10.8G    0.0237   0.01018  0.002038        64       640: 100% 254/254 [04:07<00:00,  1.03it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.17it/s]\n",
            "                 all        407        444      0.962      0.921      0.961      0.695\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/19     10.8G   0.02453   0.01036  0.002238        75       640: 100% 254/254 [04:02<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.13it/s]\n",
            "                 all        407        444      0.942      0.921      0.937      0.643\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/19     10.8G   0.02685   0.01117   0.00294        75       640: 100% 254/254 [04:02<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.09it/s]\n",
            "                 all        407        444      0.928      0.847      0.893      0.578\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/19     11.8G   0.02742   0.01154  0.003378        78       640: 100% 254/254 [04:05<00:00,  1.04it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.23it/s]\n",
            "                 all        407        444      0.907      0.818      0.914       0.61\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/19     11.8G   0.02694   0.01141  0.003098        66       640: 100% 254/254 [04:00<00:00,  1.06it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.18it/s]\n",
            "                 all        407        444       0.92      0.904      0.917      0.625\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/19     11.8G   0.02625   0.01127  0.002742        75       640: 100% 254/254 [03:59<00:00,  1.06it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:06<00:00,  1.14it/s]\n",
            "                 all        407        444      0.931      0.909      0.932      0.636\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/19     11.8G   0.02576   0.01107  0.002725        62       640: 100% 254/254 [04:00<00:00,  1.05it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 7/7 [00:05<00:00,  1.24it/s]\n",
            "                 all        407        444       0.89      0.869      0.931      0.651\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/19     11.8G   0.02567   0.01095  0.002856        97       640:  59% 151/254 [02:22<01:43,  1.00s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Chx4Rx7t4vv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pRRcIxg1FAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXP0HBjc_T3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sING_orx1E-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}